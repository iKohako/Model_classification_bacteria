# -*- coding: utf-8 -*-
"""vgg16bac.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wn40X5lHalaMkc5ODUwTTjX-G77S_6Fh
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import cv2
data_dir = ' /content/drive/MyDrive/augment'       # Corrected directory name
CATEGORIES = ['/content/drive/MyDrive/augment/anthracis','/content/drive/MyDrive/augment/melitensis','/content/drive/MyDrive/augment/pseudomallei','/content/drive/MyDrive/augment/salmonella'] # Corrected category names


for category in CATEGORIES:
  path = os.path.join(data_dir , category)
  images = os.listdir(path)  # Now this should work with the corrected directory and category names

  fig , ax = plt.subplots(1, 4, figsize=(10, 4))
  fig.suptitle(f'{category}', fontsize = 18)

  for i in range(4):
            img_name = images[np.random.randint(0, len(images))]
            img_path = os.path.join(path, img_name)
            img_array = cv2.imread(img_path)

            ax[i].imshow( img_array)
            ax[i].axis('off')

IMG_SIZE = 224
new_array = cv2.resize(img_array , (IMG_SIZE , IMG_SIZE))
plt.imshow(new_array)

new_array.shape

# https://pypi.python.org/pypi/pydot
!apt-get -qq install -y graphviz && pip install pydot
import pydot

from fastai import *
from fastai.vision import *
from fastai.metrics import accuracy

def get_databunch(tranform) :
  batchsize = 32
  sample = 5000
  np.random.seed(42)
  regex_pattern = r'.*\.jpg$'
  data = ImageDataBunch.from_folder(data_dir,
                                    random.sample(CATEGORIES, sample),
                                    regex_pattern ,
                                    ds_tfms = tranform,
                                    size = 224,
                                    ds = batchsize,

                                    )
  def get_ex() : return open_image(f'{data_dir}')

  def plots_f(row , cols , width, height , **kwargs) :
    [get_ex().apply_tfms(tranform[0] , **kwargs).show(ax=ax) for i , ax in enumerate(plt.subplots(row , cols , figsize = (width , height))[1].flatten())]
  return data

"""labeling data"""

#creating training data
training_data = []

def create_training_data():
 for category in CATEGORIES:
       path = os.path.join(data_dir , category)
       labels = CATEGORIES.index(category)
       for img in os.listdir(path):
         try:
          img_array = cv2.imread(os.path.join(path , img))
          new_array = cv2.resize(img_array , (IMG_SIZE , IMG_SIZE))
          training_data.append([new_array , labels])
         except Exception as e:
          pass


create_training_data()

len(training_data)

x = []
y = []

for features , labels in training_data:
  x.append(features)
  y.append(labels)

type(x) , type(y)

x = np.array(x).reshape(-1 , IMG_SIZE , IMG_SIZE , 3)
y = np.array(y)

type(x) , type(y)

from sklearn.model_selection import train_test_split
x_train , x_test , y_train , y_test = train_test_split(x , y , test_size = 0.2 , random_state = 42)

print(f'x_train Length : {x_train.shape[0]} , x_train Image size : {x_train.shape[1:3]}, x_train Channel Dimension : {x_train.shape[3]}')
print(f'x_test Length : {x_test.shape[0]} , x_test Image size : {x_test.shape[1:3]}, x_test Channel Dimension : {x_test.shape[3]}')

import numpy as np

np.save('x_train.npy' , np.array(x_train))
np.save('x_test.npy' , np.array(x_test))

np.save('/content/drive/MyDrive/x_train.npy' , np.array(x_train))
np.save('/content/drive/MyDrive/x_test.npy' , np.array(x_test))
print('ok')

from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input

"""loading model"""

base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

base_model.trainable = False

x_train = preprocess_input(x_train)
x_test = preprocess_input(x_test)

base_model.summary()

#add our leye
from tensorflow.keras.layers import Layer
from tensorflow.keras import models
from tensorflow.keras import layers

 # Import models from the correct location

flatten_layer = layers.Flatten()
dense_layer_1 = layers.Dense(50 , activation = 'relu')
dense_layer_2 = layers.Dense(20 , activation = 'relu')
prediction_layer = layers.Dense(4 , activation = 'softmax')

model = models.Sequential([
    base_model,
    flatten_layer,
    dense_layer_1,
    dense_layer_2,
    prediction_layer
  ])

from tensorflow.keras.callbacks import EarlyStopping

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5,  restore_best_weights=True)

model.fit(x_train , y_train , epochs = 10 , validation_data = (x_test , y_test) , callbacks = [es])

loss , accuracy = model.evaluate(x_test , y_test)
print(f'loss : {loss} , accuracy : {accuracy * 100}')

pred = np.argmax(model.predict(x_test) , axis = 1)

pred

from sklearn.metrics import classification_report , confusion_matrix
print(classification_report(y_test , pred))

fig1 = plt.gcf()
plt.plot(['accuracy'])
plt.plot(['val_accuracy'])
plt.axis(ymin=0.4,ymax=1)
plt.grid()
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend(['train', 'validation'])
plt.show()

cf = confusion_matrix(y_test , pred , normalize = 'true')
import seaborn as sns
sns.heatmap(cf , annot = True , cmap = 'crest')
plt.xlabel('Predicted');
plt.ylabel('Actual');

model.save('model.h5') # Save the model
from google.colab import files
files.download('model.h5') # Download the file using the correct name

model.save('/content/drive/MyDrive/model_vgg16bac.h5')

import matplotlib.pyplot as plt

# Check available keys in the history
print(model.history.history.keys())

# Use 'accuracy' instead of 'acc'
plt.plot(model.history.history['accuracy'])
plt.plot(model.history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')

# Fix typo in the savefig function
plt.savefig('model_accuracy.png')
files.download('model_accuracy.png')

loss , accuracy = model.evaluate(x_test , y_test)
pred = np.argmax(model.predict(x_test) , axis = 1)

fig1 = plt.gcf()
plt.plot(['accuracy'])
plt.plot(['val_accuracy'])
plt.axis(ymin=0.4,ymax=1)
plt.grid()
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend(['train', 'validation'])
plt.show()

import pandas as pd
metrics = pd.DataFrame(model.history.history)
metrics.head()

from matplotlib import pyplot as plt
import seaborn as sns
def _plot_series(series, series_name, series_index=0):
  palette = list(sns.palettes.mpl_palette('Dark2'))
  counted = (series['index']
                .value_counts()
              .reset_index(name='counts')
              .rename({'index': 'index'}, axis=1)
              .sort_values('index', ascending=True))
  xs = counted['index']
  ys = counted['counts']
  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])

fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')
df_sorted = _df_0.sort_values('index', ascending=True)
_plot_series(df_sorted, '')
sns.despine(fig=fig, ax=ax)
plt.xlabel('index')
_ = plt.ylabel('count()')